{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Problem 2\n",
    "\n",
    "reward = np.array([10, 0, 3])\n",
    "value_ = np.array(3)\n",
    "P = np.array([\n",
    "    [0.6, 0.1, 0.3],\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.4, 0.2, 0.4],\n",
    "])\n",
    "\n",
    "I = np.eye(3)\n",
    "gamma_discount= 0.9\n",
    "\n",
    "value_ = np.linalg.inv(I - gamma_discount*P)@reward\n",
    "value_ = np.round(value_, 2)\n",
    "print(\"Final Value State Function: \\n\",value_)\n",
    "\n",
    "def compute_value(P, reward, gamma_discount):\n",
    "  value_ = np.array(3)\n",
    "  I = np.eye(3)\n",
    "  gamma_discount= 0.9\n",
    "  value_ = np.linalg.inv(I - gamma_discount*P)@reward\n",
    "  value_ = np.round(value_, 2)\n",
    "  print(\"Final Value State Function: \\n\",value_)\n",
    "  return value_\n",
    "\n",
    "#Problem 3\n",
    "\n",
    "P_a0 = np.array([\n",
    "    [0.5, 0.15, 0.35],\n",
    "    [0.25, 0.4, 0.35],\n",
    "    [0.35, 0.25, 0.4]\n",
    "])\n",
    "\n",
    "\n",
    "P_a1 = np.array([\n",
    "    [0.7, 0.05, 0.25],\n",
    "    [0.35, 0.3, 0.35],\n",
    "    [0.45, 0.1, 0.45]\n",
    "])\n",
    "\n",
    "P_a2 = np.array([\n",
    "    [0.4, 0.35, 0.25],\n",
    "    [0.2, 0.6, 0.2],\n",
    "    [0.3, 0.4, 0.3]\n",
    "])\n",
    "\n",
    "R_a0 = reward\n",
    "R_a1 = np.array([15, 0, 5])\n",
    "\n",
    "R_a2 = np.array([8, 0, 2])\n",
    "\n",
    "#Added this to use in my policy_iteration code\n",
    "R_stack = np.stack([R_a0, R_a1, R_a2], axis=0)  \n",
    "\n",
    "policy_ = np.array([0.35, 0.45, 0.20])\n",
    "\n",
    "\n",
    "value_ = np.array([63.24, 49.55, 54.2])\n",
    "\n",
    "\n",
    "print(\"Action0 Transition Matrix\\n\")\n",
    "print(P_a0)\n",
    "\n",
    "print(\"\\nAction1 Transition Matrix\\n\")\n",
    "print(P_a1)\n",
    "\n",
    "print(\"\\nAction2 Transition Matrix\\n\")\n",
    "print(P_a2)\n",
    "\n",
    "print(\"\\nRewards for Action0\")\n",
    "print(R_a0)\n",
    "\n",
    "print(\"\\nRewards for Action1\")\n",
    "print(R_a1)\n",
    "\n",
    "print(\"\\nRewards for Action2\")\n",
    "print(R_a2)\n",
    "\n",
    "print(\"\\nPolicy\")\n",
    "print(policy_)\n",
    "\n",
    "print(\"\\nState Values\")\n",
    "print(value_)\n",
    "\n",
    "P_stacked = np.stack([P_a0, P_a1, P_a2], axis=0)\n",
    "P_avg = np.tensordot(policy_, P_stacked, axes=(0, 0))\n",
    "print(\"\\nWeighted Transition matrix by the policy\\n\")\n",
    "print(P_avg)\n",
    "R_avg = (policy_[0]* R_a0 + policy_[1] * R_a1 +policy_[2]*R_a2)\n",
    "print(\"\\nWeighted Reward vector by the policy\\n\")\n",
    "print(R_avg)\n",
    "V = np.linalg.inv(I - gamma_discount*P_avg)@R_avg\n",
    "V = np.round(V, 2)\n",
    "# Q = R_avg.T + ((gamma_discount*P_avg)@V.T)\n",
    "# Q = np.round(Q, 2)\n",
    "\n",
    "Q = np.zeros((3, 3))\n",
    "for a in range(3):\n",
    "    if a == 0:\n",
    "        P = P_a0\n",
    "        R = R_a0\n",
    "    elif a == 1:\n",
    "        P = P_a1\n",
    "        R = R_a1\n",
    "    elif a == 2:\n",
    "        P = P_a2\n",
    "        R = R_a2\n",
    "    Q[:, a] = R + gamma_discount * np.dot(P, V)\n",
    "\n",
    "\n",
    "Q = np.round(Q, 2)\n",
    "print(\"\\nWeighted State-Value Function by the policy\\n\")\n",
    "print(V)\n",
    "print(\"\\nWeighted Action-Value Function by the policy\\n\")\n",
    "print(Q)\n",
    "\n",
    "\n",
    "I am creating a function to calculate the Q value specific to the\n",
    "assignment, for further use.\n",
    "\n",
    "def compute_Q_value(V, P, P_a0, P_a1, P_a2):\n",
    "  Q = np.zeros((3, 3))\n",
    "  for a in range(3):\n",
    "      if a == 0:\n",
    "          P = P_a0\n",
    "          R = R_a0\n",
    "      elif a == 1:\n",
    "          P = P_a1\n",
    "          R = R_a1\n",
    "      elif a == 2:\n",
    "          P = P_a2\n",
    "          R = R_a2\n",
    "      Q[:, a] = R + gamma_discount * np.dot(P, V)\n",
    "  Q = np.round(Q, 2)\n",
    "  return Q\n",
    "\n",
    "Now let us get the optimal policy based on the Q matrix\n",
    "\n",
    "policy_ = np.argmax(Q, axis=1)\n",
    "optimal_values = Q[np.arange(len(policy_)), policy_]\n",
    "print(optimal_values)\n",
    "\n",
    "Turns out taking the action 1 at all 3 states is the best policy\n",
    "\n",
    "# visited = set()\n",
    "\n",
    "# def compute_state_value(state, visited=set()):\n",
    "#     if policy_<= policy_new:\n",
    "#         return 0\n",
    "#     if state in visited:\n",
    "#         return 0\n",
    "#     visited.add(state)\n",
    "#     action = policy.get(state, 0)\n",
    "\n",
    "#     if state not in states or action not in actions:\n",
    "#         return 0\n",
    "#     #probability for each action (policy?), I use the unwrapped for gym\n",
    "#     # transition = env.unwrapped.P[state][action]\n",
    "#     state_value = 0\n",
    "\n",
    "#     for prob, next_state, reward, done in transition:\n",
    "#         state_value += prob * (reward + gamma * compute_state_value(next_state, visited.copy()))\n",
    "\n",
    "#     return state_valuea\n",
    "\n",
    "gamma =0.8\n",
    "terminal_state = 14\n",
    "def compute_q_values(state, action):\n",
    "  if state == terminal_state:\n",
    "    return 0 \n",
    "\n",
    "  q_value = 0\n",
    "  for prob, next_state, reward, terminated in env.unwrapped.P[state][action]:\n",
    "      q_value += prob * (reward + gamma * compute_state_value(next_state) if not terminated else reward)\n",
    "\n",
    "  return q_value\n",
    "\n",
    "num_states = P_avg.shape[0]\n",
    "num_actions = P_avg.shape[1]\n",
    "print(\"Number of States: \", num_states)\n",
    "print(\"Number of States: \", num_actions)\n",
    "\n",
    "#Policy Evaluation\n",
    "\n",
    "def policy_evaluation(policy):\n",
    "    V = {state: compute_state_value(state) for state in range(num_states)}\n",
    "    return V\n",
    "\n",
    "#Policy Improvement\n",
    "\n",
    "def policy_improvement(policy):\n",
    "    improved_policy = {s: 0 for s in range(num_states-1)} \n",
    "    Q = {(state, action): compute_q_value(state, action, policy) for state in range(num_states) for action in range(num_actions)}\n",
    "    for state in range(num_states-1):\n",
    "        max_action = max(range(num_actions), key=lambda action:Q[(state, action)] )\n",
    "        improved_policy[state] = max_action\n",
    "\n",
    "    return improved_policy\n",
    "\n",
    "#Polic Iteration\n",
    "*Putting Evaluation and Improvement Together*\n",
    "\n",
    "def policy_iteration():\n",
    "    policy = {0:2, 1:2, 2:1, 3:1, 4:0, 5:0, 6:2, 7:2}\n",
    "    while True:\n",
    "        V = policy_evaluation(policy)\n",
    "        improved_policy = policy_improvement(policy)\n",
    "        if improved_policy == policy:\n",
    "            break\n",
    "        policy = improved_policy\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "policy, V = policy_iteration()\n",
    "\n",
    "def value_iteration():\n",
    "    if state == terminal_state:\n",
    "        return 0 \n",
    "    action = policy.get(state, 0)\n",
    "    if state not in env.unwrapped.P or action not in env.unwrapped.P[state]:\n",
    "        return 0\n",
    "    state_transitions = env.unwrapped.P[state][action]  # List of (prob, next_state, reward, done) tuples\n",
    "\n",
    "    expected_value = 0  \n",
    "    print(\"Final Value State Function: \\n\",value_)\n",
    "    for prob, next_state, reward, done in state_transitions:\n",
    "        expected_value += prob * (reward + gamma * compute_state_value(next_state)) #if not done else reward)\n",
    "\n",
    "    return expected_value\n",
    "\n",
    "#Richardson Iteration\n",
    "\n",
    "I created an episodic task sample to test my function on, since the Problems1-3 are not episodic, but continuous.\n",
    "The code below is the general case.\n",
    "\n",
    "def richardson_iteration_value_function(P, R, gamma, max_iterations=1000, tolerance=1e-6):\n",
    "    n_states = P.shape[0]\n",
    "    value_ = np.zeros(n_states) \n",
    "    deltas = []\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        value_new = R + gamma * np.dot(P, value_)\n",
    "        delta = np.max(np.abs(value_new - value_))\n",
    "        deltas.append(delta)\n",
    "        value_ = value_new\n",
    "        if delta < tolerance:\n",
    "            print(f\"Converged after {i+1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return value_, deltas\n",
    "\n",
    "value_, deltas = richardson_iteration_value_function(P_avg, R_avg, gamma_discount)\n",
    "value_ = np.round(value_, 2)\n",
    "print(\"\\nWeighted State-Value Function by the policy\\n\")\n",
    "print(value_)\n",
    "\n",
    "Note that I got exactly the same State-Value function as in the Problem 3. As instructed, I will create a more general maze case, to test my richardson iteration on.\n",
    "\n",
    "\n",
    "\n",
    "def my_mdp():\n",
    "\n",
    "    P = np.array([\n",
    "        [0.7, 0.3, 0.0, 0.0],\n",
    "        [0.1, 0.6, 0.3, 0.0],\n",
    "        [0.0, 0.1, 0.7, 0.2],\n",
    "        [0.0, 0.0, 0.0, 1.0]\n",
    "    ])\n",
    "\n",
    "    R = np.array([0, 0, 0, 10])\n",
    "\n",
    "    gamma = 0.9\n",
    "\n",
    "    return P, R, gamma\n",
    "\n",
    "P, R, gamma = my_mdp()\n",
    "V, errors = richardson_iteration_value_function(P, R, gamma)\n",
    "\n",
    "print(\"Final value function:\")\n",
    "print(V)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(errors)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Error (Delta)')\n",
    "plt.title('Richardson Iteration Convergence')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "I = np.eye(P.shape[0])\n",
    "V_analytical = np.linalg.solve(I - gamma * P, R)\n",
    "print(\"\\nAnalytical solution:\")\n",
    "print(V_analytical)\n",
    "print(f\"Difference: {np.max(np.abs(V - V_analytical))}\")\n",
    "\n",
    "#Richardson Iteration\n",
    "\n",
    "def policy_evaluation_richardson(P_stacked, R_stack, policy, gamma, max_iter=1000, tol=1e-6):\n",
    "    n_states = P_stacked.shape[1]\n",
    "    V = np.zeros(n_states)  # Initialize value function\n",
    "\n",
    "    # Convert deterministic policy indices to transition matrix and reward vector\n",
    "    P_policy = np.zeros((n_states, n_states))\n",
    "    R_policy = np.zeros(n_states)\n",
    "\n",
    "    for s in range(n_states):\n",
    "        a = policy[s]\n",
    "        P_policy[s] = P_stacked[a, s]\n",
    "        R_policy[s] = R_stack[a, s]\n",
    "\n",
    "    # Richardson iteration\n",
    "    for i in range(max_iter):\n",
    "        delta = 0\n",
    "        # Perform Bellman update: V_new = R + gamma * P * V\n",
    "        V_new = R_policy + gamma * np.dot(P_policy, V)\n",
    "\n",
    "        # Check convergence\n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "\n",
    "        if delta < tol:\n",
    "            print(f\"Policy evaluation converged after {i+1} iterations\")\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "# Problem 2: Policy Iteration\n",
    "\n",
    "def policy_iteration(P_stack, R_stack, gamma, max_iter=100, eval_max_iter=1000, tol=1e-6):\n",
    "    n_states = P_stack.shape[1]\n",
    "    n_actions = P_stack.shape[0]\n",
    "\n",
    "    # Initialize a random policy\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 1. Policy Evaluation\n",
    "        V = policy_evaluation_richardson(P_stack, R_stack, policy, gamma, eval_max_iter, tol)\n",
    "\n",
    "        # 2. Policy Improvement\n",
    "        policy_stable = True\n",
    "\n",
    "        for s in range(n_states):\n",
    "            old_action = policy[s]\n",
    "\n",
    "            # Calculate Q-values for all actions in this state\n",
    "            Q_s = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                Q_s[a] = R_stack[a, s] + gamma * np.dot(P_stack[a, s], V)\n",
    "\n",
    "            # Choose the best action\n",
    "            policy[s] = np.argmax(Q_s)\n",
    "\n",
    "            # Check if policy changed\n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "\n",
    "        print(f\"Iteration {i+1}, Policy: {policy}\")\n",
    "\n",
    "        # If policy is stable, we've found the optimal policy\n",
    "        if policy_stable:\n",
    "            print(f\"Policy iteration converged after {i+1} iterations\")\n",
    "            break\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Problem 3: Value Iteration\n",
    "\n",
    "def value_iteration(P_stack, R_stack, gamma, max_iter=1000, tol=1e-6):\n",
    "    n_states = P_stack.shape[1]\n",
    "    n_actions = P_stack.shape[0]\n",
    "\n",
    "    # Initialize value function\n",
    "    V = np.zeros(n_states)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        delta = 0\n",
    "\n",
    "        # Bellman optimality update\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "\n",
    "            # Calculate maximum Q-value across all actions\n",
    "            Q_s = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                Q_s[a] = R_stack[a, s] + gamma * np.dot(P_stack[a, s], V)\n",
    "\n",
    "            # Update state value with maximum Q-value\n",
    "            V[s] = np.max(Q_s)\n",
    "\n",
    "            # Track maximum change\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "\n",
    "        # Check convergence\n",
    "        if delta < tol:\n",
    "            print(f\"Value iteration converged after {i+1} iterations\")\n",
    "            break\n",
    "\n",
    "    # Extract policy from value function\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    for s in range(n_states):\n",
    "        Q_s = np.zeros(n_actions)\n",
    "        for a in range(n_actions):\n",
    "            Q_s[a] = R_stack[a, s] + gamma * np.dot(P_stack[a, s], V)\n",
    "        policy[s] = np.argmax(Q_s)\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "print(\"\\n--- Problem 1: Policy Evaluation ---\")\n",
    "V_eval = policy_evaluation_richardson(P_stacked, R_stack, deterministic_policy, gamma)\n",
    "print(\"Value function for policy [1, 1, 1]:\")\n",
    "print(V_eval)\n",
    "\n",
    "print(\"\\n--- Problem 2: Policy Iteration ---\")\n",
    "optimal_policy_pi, V_pi = policy_iteration(P_stacked, R_stack, gamma)\n",
    "print(\"Optimal policy from Policy Iteration:\")\n",
    "print(optimal_policy_pi)\n",
    "print(\"Optimal value function:\")\n",
    "print(V_pi)\n",
    "\n",
    "print(\"\\n--- Problem 3: Value Iteration ---\")\n",
    "optimal_policy_vi, V_vi = value_iteration(P_stacked, R_stack, gamma)\n",
    "print(\"Optimal policy from Value Iteration:\")\n",
    "print(optimal_policy_vi)\n",
    "print(\"Optimal value function:\")\n",
    "print(V_vi)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(\"Are the optimal policies the same?\", np.array_equal(optimal_policy_pi, optimal_policy_vi))\n",
    "print(\"Maximum difference in value functions:\", np.max(np.abs(V_pi - V_vi)))\n",
    "\n",
    "# Calculate Q-values for the optimal policy\n",
    "Q_optimal = np.zeros((n_states, n_actions))\n",
    "for s in range(n_states):\n",
    "    for a in range(n_actions):\n",
    "        Q_optimal[s, a] = R_stack[a, s] + gamma * np.dot(P_stacked[a, s], V_pi)\n",
    "\n",
    "print(\"\\nOptimal Q-values:\")\n",
    "print(np.round(Q_optimal, 2))\n",
    "print(\"Optimal policy actions:\")\n",
    "print(np.argmax(Q_optimal, axis=1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
