{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSNUR1K8kL3B"
      },
      "source": [
        "# RL: Markov Decision Process\n",
        "Author: Arin Avanoosyan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pScZpqrBIK4e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ux-WhhkFih"
      },
      "source": [
        "#Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2B7Xp1pHfsz",
        "outputId": "687c0bb5-333b-45ac-81b8-b1d2da3c06c2"
      },
      "outputs": [],
      "source": [
        "reward = np.array([10, 0, 3])\n",
        "value_ = np.array(3)\n",
        "P = np.array([\n",
        "    [0.6, 0.1, 0.3],\n",
        "    [0.3, 0.4, 0.3],\n",
        "    [0.4, 0.2, 0.4],\n",
        "])\n",
        "\n",
        "I = np.eye(3)\n",
        "gamma_discount= 0.9\n",
        "\n",
        "value_ = np.linalg.inv(I - gamma_discount*P)@reward\n",
        "value_ = np.round(value_, 2)\n",
        "print(\"Final Value State Function: \\n\",value_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubykdzx1Q5Rw"
      },
      "outputs": [],
      "source": [
        "def compute_value(P, reward, gamma_discount):\n",
        "  value_ = np.array(3)\n",
        "  I = np.eye(3)\n",
        "  gamma_discount= 0.9\n",
        "  value_ = np.linalg.inv(I - gamma_discount*P)@reward\n",
        "  value_ = np.round(value_, 2)\n",
        "  print(\"Final Value State Function: \\n\",value_)\n",
        "  return value_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhwqguCakC3W"
      },
      "source": [
        "#Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFzfRWLUfXQi",
        "outputId": "8c07ea67-4d3f-4b68-9c72-9cbf8ee1b795"
      },
      "outputs": [],
      "source": [
        "P_a0 = np.array([\n",
        "    [0.5, 0.15, 0.35],\n",
        "    [0.25, 0.4, 0.35],\n",
        "    [0.35, 0.25, 0.4]\n",
        "])\n",
        "\n",
        "\n",
        "P_a1 = np.array([\n",
        "    [0.7, 0.05, 0.25],\n",
        "    [0.35, 0.3, 0.35],\n",
        "    [0.45, 0.1, 0.45]\n",
        "])\n",
        "\n",
        "P_a2 = np.array([\n",
        "    [0.4, 0.35, 0.25],\n",
        "    [0.2, 0.6, 0.2],\n",
        "    [0.3, 0.4, 0.3]\n",
        "])\n",
        "\n",
        "R_a0 = reward\n",
        "R_a1 = np.array([15, 0, 5])\n",
        "\n",
        "R_a2 = np.array([8, 0, 2])\n",
        "\n",
        "#Added this to use in my policy_iteration code\n",
        "R_stack = np.stack([R_a0, R_a1, R_a2], axis=0)\n",
        "\n",
        "policy_ = np.array([0.35, 0.45, 0.20])\n",
        "\n",
        "\n",
        "value_ = np.array([63.24, 49.55, 54.2])\n",
        "\n",
        "\n",
        "print(\"Action0 Transition Matrix\\n\")\n",
        "print(P_a0)\n",
        "\n",
        "print(\"\\nAction1 Transition Matrix\\n\")\n",
        "print(P_a1)\n",
        "\n",
        "print(\"\\nAction2 Transition Matrix\\n\")\n",
        "print(P_a2)\n",
        "\n",
        "print(\"\\nRewards for Action0\")\n",
        "print(R_a0)\n",
        "\n",
        "print(\"\\nRewards for Action1\")\n",
        "print(R_a1)\n",
        "\n",
        "print(\"\\nRewards for Action2\")\n",
        "print(R_a2)\n",
        "\n",
        "print(\"\\nPolicy\")\n",
        "print(policy_)\n",
        "\n",
        "print(\"\\nState Values\")\n",
        "print(value_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6pzw3ZouQ4s",
        "outputId": "a7c14ee9-ef67-42b8-baf5-e4fafdd0210a"
      },
      "outputs": [],
      "source": [
        "P_stacked = np.stack([P_a0, P_a1, P_a2], axis=0)\n",
        "P_avg = np.tensordot(policy_, P_stacked, axes=(0, 0))\n",
        "print(\"\\nWeighted Transition matrix by the policy\\n\")\n",
        "print(P_avg)\n",
        "R_avg = (policy_[0]* R_a0 + policy_[1] * R_a1 +policy_[2]*R_a2)\n",
        "print(\"\\nWeighted Reward vector by the policy\\n\")\n",
        "print(R_avg)\n",
        "V = np.linalg.inv(I - gamma_discount*P_avg)@R_avg\n",
        "V = np.round(V, 2)\n",
        "# Q = R_avg.T + ((gamma_discount*P_avg)@V.T)\n",
        "# Q = np.round(Q, 2)\n",
        "\n",
        "Q = np.zeros((3, 3))\n",
        "for a in range(3):\n",
        "    if a == 0:\n",
        "        P = P_a0\n",
        "        R = R_a0\n",
        "    elif a == 1:\n",
        "        P = P_a1\n",
        "        R = R_a1\n",
        "    elif a == 2:\n",
        "        P = P_a2\n",
        "        R = R_a2\n",
        "    Q[:, a] = R + gamma_discount * np.dot(P, V)\n",
        "\n",
        "\n",
        "Q = np.round(Q, 2)\n",
        "print(\"\\nWeighted State-Value Function by the policy\\n\")\n",
        "print(V)\n",
        "print(\"\\nWeighted Action-Value Function by the policy\\n\")\n",
        "print(Q)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYVZtj61YVui"
      },
      "source": [
        "I am creating a function to calculate the Q value specific to the\n",
        "assignment, for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GZTtMhOYUoK"
      },
      "outputs": [],
      "source": [
        "def compute_Q_value(V, P, P_a0, P_a1, P_a2):\n",
        "  Q = np.zeros((3, 3))\n",
        "  for a in range(3):\n",
        "      if a == 0:\n",
        "          P = P_a0\n",
        "          R = R_a0\n",
        "      elif a == 1:\n",
        "          P = P_a1\n",
        "          R = R_a1\n",
        "      elif a == 2:\n",
        "          P = P_a2\n",
        "          R = R_a2\n",
        "      Q[:, a] = R + gamma_discount * np.dot(P, V)\n",
        "  Q = np.round(Q, 2)\n",
        "  return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO3QOYSfK1dV"
      },
      "source": [
        "Now let us get the optimal policy based on the Q matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLwjw0NMHjiH",
        "outputId": "51968a36-47de-4031-90ce-2ae83eaa06c5"
      },
      "outputs": [],
      "source": [
        "policy_ = np.argmax(Q, axis=1)\n",
        "optimal_values = Q[np.arange(len(policy_)), policy_]\n",
        "print(optimal_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq07X8JUK53R"
      },
      "source": [
        "Turns out taking the action 1 at all 3 states is the best policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7-ImuzoLYNW"
      },
      "outputs": [],
      "source": [
        "# visited = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4TsboEGIJi9"
      },
      "outputs": [],
      "source": [
        "# def compute_state_value(state, visited=set()):\n",
        "#     if policy_<= policy_new:\n",
        "#         return 0\n",
        "#     if state in visited:\n",
        "#         return 0\n",
        "#     visited.add(state)\n",
        "#     action = policy.get(state, 0)\n",
        "\n",
        "#     if state not in states or action not in actions:\n",
        "#         return 0\n",
        "#     #probability for each action (policy?), I use the unwrapped for gym\n",
        "#     # transition = env.unwrapped.P[state][action]\n",
        "#     state_value = 0\n",
        "\n",
        "#     for prob, next_state, reward, done in transition:\n",
        "#         state_value += prob * (reward + gamma * compute_state_value(next_state, visited.copy()))\n",
        "\n",
        "#     return state_valuea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFuUtMztUu5o"
      },
      "outputs": [],
      "source": [
        "gamma =0.8\n",
        "terminal_state = 14\n",
        "def compute_q_values(state, action):\n",
        "  if state == terminal_state:\n",
        "    return 0\n",
        "\n",
        "  q_value = 0\n",
        "  for prob, next_state, reward, terminated in env.unwrapped.P[state][action]:\n",
        "      q_value += prob * (reward + gamma * compute_state_value(next_state) if not terminated else reward)\n",
        "\n",
        "  return q_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAKBHxazXiZO",
        "outputId": "7010bb44-7df0-4735-92c7-9c6de15d8bd5"
      },
      "outputs": [],
      "source": [
        "num_states = P_avg.shape[0]\n",
        "num_actions = P_avg.shape[1]\n",
        "print(\"Number of States: \", num_states)\n",
        "print(\"Number of States: \", num_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ocm7IW20OKVJ"
      },
      "source": [
        "#Policy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0tAnohh6UwT"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(policy):\n",
        "    V = {state: compute_state_value(state) for state in range(num_states)}\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloAwWi6OPrW"
      },
      "source": [
        "#Policy Improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTW5Jy1K6Xzy"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(policy):\n",
        "    improved_policy = {s: 0 for s in range(num_states-1)}\n",
        "    Q = {(state, action): compute_q_value(state, action, policy) for state in range(num_states) for action in range(num_actions)}\n",
        "    for state in range(num_states-1):\n",
        "        max_action = max(range(num_actions), key=lambda action:Q[(state, action)] )\n",
        "        improved_policy[state] = max_action\n",
        "\n",
        "    return improved_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSBGChO2OTmd"
      },
      "source": [
        "#Polic Iteration\n",
        "*Putting Evaluation and Improvement Together*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm5dcTwMUUxB"
      },
      "outputs": [],
      "source": [
        "def policy_improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "dFV7qcds6UCG",
        "outputId": "913d45ff-9fd4-46af-e539-5f3636564100"
      },
      "outputs": [],
      "source": [
        "def policy_iteration():\n",
        "    policy = {0:2, 1:2, 2:1, 3:1, 4:0, 5:0, 6:2, 7:2}\n",
        "    while True:\n",
        "        V = policy_evaluation(policy)\n",
        "        improved_policy = policy_improvement(policy)\n",
        "        if improved_policy == policy:\n",
        "            break\n",
        "        policy = improved_policy\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "policy, V = policy_iteration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "HpvmwUEP6tD6",
        "outputId": "8acaf698-46ec-4c4b-be62-c1a02304a54c"
      },
      "outputs": [],
      "source": [
        "def value_iteration():\n",
        "    if state == terminal_state:\n",
        "        return 0\n",
        "    action = policy.get(state, 0)\n",
        "    if state not in env.unwrapped.P or action not in env.unwrapped.P[state]:\n",
        "        return 0\n",
        "    state_transitions = env.unwrapped.P[state][action]  # List of (prob, next_state, reward, done) tuples\n",
        "\n",
        "    expected_value = 0  \n",
        "    print(\"Final Value State Function: \\n\",value_)\n",
        "    for prob, next_state, reward, done in state_transitions:\n",
        "        expected_value += prob * (reward + gamma * compute_state_value(next_state)) #if not done else reward)\n",
        "\n",
        "    return expected_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OCZfr1lQcqH"
      },
      "source": [
        "#Richardson Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxlUaklmQghB"
      },
      "source": [
        "I created an episodic task sample to test my function on, since the Problems1-3 are not episodic, but continuous.\n",
        "The code below is the general case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAesrz8RfJHR",
        "outputId": "b3bf9f6f-5523-4a13-8b1a-fe7b4fa334a0"
      },
      "outputs": [],
      "source": [
        "def richardson_iteration_value_function(P, R, gamma, max_iterations=1000, tolerance=1e-6):\n",
        "    n_states = P.shape[0]\n",
        "    value_ = np.zeros(n_states)\n",
        "    deltas = []\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        value_new = R + gamma * np.dot(P, value_)\n",
        "        delta = np.max(np.abs(value_new - value_))\n",
        "        deltas.append(delta)\n",
        "        value_ = value_new\n",
        "        if delta < tolerance:\n",
        "            print(f\"Converged after {i+1} iterations\")\n",
        "            break\n",
        "\n",
        "    return value_, deltas\n",
        "\n",
        "value_, deltas = richardson_iteration_value_function(P_avg, R_avg, gamma_discount)\n",
        "value_ = np.round(value_, 2)\n",
        "print(\"\\nWeighted State-Value Function by the policy\\n\")\n",
        "print(value_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYYP3Oh4gHbz"
      },
      "source": [
        "Note that I got exactly the same State-Value function as in the Problem 3. As instructed, I will create a more general maze case, to test my richardson iteration on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oWjFPHPgd_P"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "ERYdZigEQfEi",
        "outputId": "23ef5763-e0b7-4eea-8abe-e4e873e946fa"
      },
      "outputs": [],
      "source": [
        "def my_mdp():\n",
        "\n",
        "    P = np.array([\n",
        "        [0.7, 0.3, 0.0, 0.0],\n",
        "        [0.1, 0.6, 0.3, 0.0],\n",
        "        [0.0, 0.1, 0.7, 0.2],\n",
        "        [0.0, 0.0, 0.0, 1.0]\n",
        "    ])\n",
        "\n",
        "    R = np.array([0, 0, 0, 10])\n",
        "\n",
        "    gamma = 0.9\n",
        "\n",
        "    return P, R, gamma\n",
        "\n",
        "P, R, gamma = my_mdp()\n",
        "V, errors = richardson_iteration_value_function(P, R, gamma)\n",
        "\n",
        "print(\"Final value function:\")\n",
        "print(V)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(errors)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Error (Delta)')\n",
        "plt.title('Richardson Iteration Convergence')\n",
        "plt.yscale('log')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "I = np.eye(P.shape[0])\n",
        "V_analytical = np.linalg.solve(I - gamma * P, R)\n",
        "print(\"\\nAnalytical solution:\")\n",
        "print(V_analytical)\n",
        "print(f\"Difference: {np.max(np.abs(V - V_analytical))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOgHy1NJTGs0"
      },
      "source": [
        "#Richardson Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkt3hwv5S6O9"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation_richardson(P_stacked, R_stack, policy, gamma, max_iter=1000, tol=1e-6):\n",
        "    n_states = P_stacked.shape[1]\n",
        "    V = np.zeros(n_states)  # Initialize value function\n",
        "\n",
        "    # Convert deterministic policy indices to transition matrix and reward vector\n",
        "    P_policy = np.zeros((n_states, n_states))\n",
        "    R_policy = np.zeros(n_states)\n",
        "\n",
        "    for s in range(n_states):\n",
        "        a = policy[s]\n",
        "        P_policy[s] = P_stacked[a, s]\n",
        "        R_policy[s] = R_stack[a, s]\n",
        "\n",
        "    # Richardson iteration\n",
        "    for i in range(max_iter):\n",
        "        delta = 0\n",
        "        # Perform Bellman update: V_new = R + gamma * P * V\n",
        "        V_new = R_policy + gamma * np.dot(P_policy, V)\n",
        "\n",
        "        # Check convergence\n",
        "        delta = np.max(np.abs(V_new - V))\n",
        "        V = V_new\n",
        "\n",
        "        if delta < tol:\n",
        "            print(f\"Policy evaluation converged after {i+1} iterations\")\n",
        "            break\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LVcs4RZTCD6"
      },
      "source": [
        "# Problem 2: Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0plxXOES2iQ"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(P_stack, R_stack, gamma, max_iter=100, eval_max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Policy Iteration algorithm\n",
        "\n",
        "    Parameters:\n",
        "    P_stack: Transition probability matrices [n_actions, n_states, n_states]\n",
        "    R_stack: Reward vectors [n_actions, n_states]\n",
        "    gamma: Discount factor\n",
        "    max_iter: Maximum policy improvement iterations\n",
        "    eval_max_iter: Maximum policy evaluation iterations\n",
        "    tol: Convergence tolerance\n",
        "\n",
        "    Returns:\n",
        "    policy: Optimal policy [n_states]\n",
        "    V: Optimal value function [n_states]\n",
        "    \"\"\"\n",
        "    n_states = P_stack.shape[1]\n",
        "    n_actions = P_stack.shape[0]\n",
        "\n",
        "    # Initialize a random policy\n",
        "    policy = np.zeros(n_states, dtype=int)\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        # 1. Policy Evaluation\n",
        "        V = policy_evaluation_richardson(P_stack, R_stack, policy, gamma, eval_max_iter, tol)\n",
        "\n",
        "        # 2. Policy Improvement\n",
        "        policy_stable = True\n",
        "\n",
        "        for s in range(n_states):\n",
        "            old_action = policy[s]\n",
        "\n",
        "            # Calculate Q-values for all actions in this state\n",
        "            Q_s = np.zeros(n_actions)\n",
        "            for a in range(n_actions):\n",
        "                Q_s[a] = R_stack[a, s] + gamma * np.dot(P_stack[a, s], V)\n",
        "\n",
        "            # Choose the best action\n",
        "            policy[s] = np.argmax(Q_s)\n",
        "\n",
        "            # Check if policy changed\n",
        "            if old_action != policy[s]:\n",
        "                policy_stable = False\n",
        "\n",
        "        print(f\"Iteration {i+1}, Policy: {policy}\")\n",
        "\n",
        "        # If policy is stable, we've found the optimal policy\n",
        "        if policy_stable:\n",
        "            print(f\"Policy iteration converged after {i+1} iterations\")\n",
        "            break\n",
        "\n",
        "    return policy, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO_ETGRjS-eq"
      },
      "source": [
        "# Problem 3: Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzOonfGmQbzI"
      },
      "outputs": [],
      "source": [
        "def value_iteration(P_stack, R_stack, gamma, max_iter=1000, tol=1e-6):\n",
        "    n_states = P_stack.shape[1]\n",
        "    n_actions = P_stack.shape[0]\n",
        "\n",
        "    # Initialize value function\n",
        "    V = np.zeros(n_states)\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        delta = 0\n",
        "\n",
        "        # Bellman optimality update\n",
        "        for s in range(n_states):\n",
        "            v = V[s]\n",
        "\n",
        "            # Calculate maximum Q-value across all actions\n",
        "            Q_s = np.zeros(n_actions)\n",
        "            for a in range(n_actions):\n",
        "                Q_s[a] = R_stack[a, s] + gamma * np.dot(P_stack[a, s], V)\n",
        "\n",
        "            # Update state value with maximum Q-value\n",
        "            V[s] = np.max(Q_s)\n",
        "\n",
        "            # Track maximum change\n",
        "            delta = max(delta, np.abs(v - V[s]))\n",
        "\n",
        "        # Check convergence\n",
        "        if delta < tol:\n",
        "            print(f\"Value iteration converged after {i+1} iterations\")\n",
        "            break\n",
        "\n",
        "    # Extract policy from value function\n",
        "    policy = np.zeros(n_states, dtype=int)\n",
        "    for s in range(n_states):\n",
        "        Q_s = np.zeros(n_actions)\n",
        "        for a in range(n_actions):\n",
        "            Q_s[a] = R_stack[a, s] + gamma * np.dot(P_stack[a, s], V)\n",
        "        policy[s] = np.argmax(Q_s)\n",
        "\n",
        "    return policy, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "2xtMfKDqTNmj",
        "outputId": "1ddd7cbc-c002-40e4-8c52-ccc6af05cb82"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Problem 1: Policy Evaluation ---\")\n",
        "V_eval = policy_evaluation_richardson(P_stacked, R_stack, deterministic_policy, gamma)\n",
        "print(\"Value function for policy [1, 1, 1]:\")\n",
        "print(V_eval)\n",
        "\n",
        "print(\"\\n--- Problem 2: Policy Iteration ---\")\n",
        "optimal_policy_pi, V_pi = policy_iteration(P_stacked, R_stack, gamma)\n",
        "print(\"Optimal policy from Policy Iteration:\")\n",
        "print(optimal_policy_pi)\n",
        "print(\"Optimal value function:\")\n",
        "print(V_pi)\n",
        "\n",
        "print(\"\\n--- Problem 3: Value Iteration ---\")\n",
        "optimal_policy_vi, V_vi = value_iteration(P_stacked, R_stack, gamma)\n",
        "print(\"Optimal policy from Value Iteration:\")\n",
        "print(optimal_policy_vi)\n",
        "print(\"Optimal value function:\")\n",
        "print(V_vi)\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(\"Are the optimal policies the same?\", np.array_equal(optimal_policy_pi, optimal_policy_vi))\n",
        "print(\"Maximum difference in value functions:\", np.max(np.abs(V_pi - V_vi)))\n",
        "\n",
        "# Calculate Q-values for the optimal policy\n",
        "Q_optimal = np.zeros((n_states, n_actions))\n",
        "for s in range(n_states):\n",
        "    for a in range(n_actions):\n",
        "        Q_optimal[s, a] = R_stack[a, s] + gamma * np.dot(P_stacked[a, s], V_pi)\n",
        "\n",
        "print(\"\\nOptimal Q-values:\")\n",
        "print(np.round(Q_optimal, 2))\n",
        "print(\"Optimal policy actions:\")\n",
        "print(np.argmax(Q_optimal, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faeTaH1XTN6R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
