{"cells":[{"source":"![Crowded city](city-1265055_1280.jpg)\n\nIn the quest for efficiency and effectiveness in urban transportation, finding the optimal routes to take passengers from their initial locations to their desired destinations is paramount. This challenge is not just about reducing travel time; it's about enhancing the overall experience for both drivers and passengers, ensuring safety, and minimizing environmental impact. \n\nYou have been asked to revolutionize the way taxis navigate the urban landscape, ensuring passengers reach their destinations swiftly, safely, and satisfactorily. As an initial step, your goal is to build a reinforcement learning agent that solves this problem within a simulated environment.\n\n## The Taxi-v3 environment\nThe Taxi-v3 environment is a strategic simulation, offering a grid-based arena where a taxi navigates to address daily challenges akin to those faced by a taxi driver. This environment is defined by a 5x5 grid where the taxi's mission involves picking up a passenger from one of four specific locations (marked as Red, Green, Yellow, and Blue) and dropping them off at another designated spot. The goal is to accomplish this with minimal time on the road to maximize rewards, emphasizing the need for route optimization and efficient decision-making for passenger pickup and dropoff.\n\n### Key Components:\n- **Action Space:** Comprises six actions where 0 moves the taxi south, 1 north, 2 east, 3 west, 4 picks up a passenger, and 5 drops off a passenger.\n- **Observation Space:** Comprises 500 discrete states, accounting for 25 taxi positions, 5 potential passenger locations, and 4 destinations. \n- **Rewards System:** Includes a penalty of -1 for each step taken without other rewards, +20 for successful passenger delivery, and -10 for illegal pickup or dropoff actions. Actions resulting in no operation, like hitting a wall, also incur a time step penalty.\n\n![Taxi-v3 environment snapshot](Taxi_snap.png)\n","metadata":{},"id":"806ef75e-0551-4d96-912d-b44a6aa5ca20","cell_type":"markdown"},{"source":"# Re-run this cell to install and import the necessary libraries and load the required variables\n!pip install gymnasium[toy_text] imageio\nimport numpy as np\nimport gymnasium as gym\nimport imageio\nfrom IPython.display import Image\nfrom gymnasium.utils import seeding\nimport matplotlib.pyplot as plt\n\n# Initialize the Taxi-v3 environment\nenv = gym.make(\"Taxi-v3\", render_mode='rgb_array')\n\n# Seed the environment for reproducibility\nenv.np_random, _ = seeding.np_random(42)\nenv.action_space.seed(42)\nnp.random.seed(42)\n\n# Maximum number of actions per training episode\nmax_actions = 100 \nnum_episodes = 100\n\n# Frames\nframes = []\n\n# Alpha and Gamma\nlearning_rate = 0.9\ngamma = 0.9","metadata":{"executionCancelledAt":null,"executionTime":9696,"lastExecutedAt":1741641359365,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Re-run this cell to install and import the necessary libraries and load the required variables\n!pip install gymnasium[toy_text] imageio\nimport numpy as np\nimport gymnasium as gym\nimport imageio\nfrom IPython.display import Image\nfrom gymnasium.utils import seeding\nimport matplotlib.pyplot as plt\n\n# Initialize the Taxi-v3 environment\nenv = gym.make(\"Taxi-v3\", render_mode='rgb_array')\n\n# Seed the environment for reproducibility\nenv.np_random, _ = seeding.np_random(42)\nenv.action_space.seed(42)\nnp.random.seed(42)\n\n# Maximum number of actions per training episode\nmax_actions = 100 \nnum_episodes = 100\n\n# Frames\nframes = []\n\n# Alpha and Gamma\nlearning_rate = 0.9\ngamma = 0.9","outputsMetadata":{"0":{"height":395,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"03387f2a-fafb-4ab9-b67d-404d4d97d654"},"id":"f59944f1-c11a-48e7-a1aa-10ede08e0ccf","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (2.21.1)\nCollecting gymnasium[toy_text]\n  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (1.23.2)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (2.1.0)\nRequirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (6.8.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (4.12.2)\nCollecting farama-notifications>=0.0.1 (from gymnasium[toy_text])\n  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\nCollecting pygame>=2.1.3 (from gymnasium[toy_text])\n  Downloading pygame-2.6.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.8/dist-packages (from imageio) (9.2.0)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium[toy_text]) (3.8.1)\nDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\nDownloading pygame-2.6.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: farama-notifications, pygame, gymnasium\nSuccessfully installed farama-notifications-0.0.4 gymnasium-1.1.1 pygame-2.6.1\n"}]},{"source":"num_states = env.observation_space.n\nnum_actions = env.action_space.n\nprint(\"\\nNumber of States: \", num_states)\nprint(\"\\nNumber of Actions: \", num_actions)\npolicy = {state: action for state in range(num_states) for action in range(num_actions)}\nprint(env.unwrapped.P[498])\n#Q Table\nq_table =np.zeros((num_states, num_actions))","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1741641359414,"lastExecutedByKernel":"03387f2a-fafb-4ab9-b67d-404d4d97d654","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"num_states = env.observation_space.n\nnum_actions = env.action_space.n\nprint(\"\\nNumber of States: \", num_states)\nprint(\"\\nNumber of Actions: \", num_actions)\npolicy = {state: action for state in range(num_states) for action in range(num_actions)}\nprint(env.unwrapped.P[498])\n#Q Table\nq_table =np.zeros((num_states, num_actions))","outputsMetadata":{"0":{"height":143,"type":"stream"}}},"cell_type":"code","id":"89d81991-be30-44f2-84bd-2682a7cf6294","outputs":[{"output_type":"stream","name":"stdout","text":"\nNumber of States:  500\n\nNumber of Actions:  6\n{0: [(1.0, 498, -1, False)], 1: [(1.0, 398, -1, False)], 2: [(1.0, 498, -1, False)], 3: [(1.0, 478, -1, False)], 4: [(1.0, 498, -10, False)], 5: [(1.0, 498, -10, False)]}\n"}],"execution_count":2},{"source":"def epsilon_greedy(state):\n    if rand <epsilon:\n        return env.action_space.sample()\n    else:\n        np.argmax(q_table[state])","metadata":{},"cell_type":"code","id":"f8d5a0a3-48e1-478f-87cb-a4bed6b745b2","outputs":[],"execution_count":null},{"source":"def update_q_table(state, action, reward, next_state):\n    q_table[state, action] = (1 - learning_rate) * q_table[state, action] + learning_rate * (reward + gamma * np.max(q_table[next_state]))\n\nepisode_total_reward = np.zeros(num_episodes)\nsave_frequency = 100 \n\nfor i in range(num_episodes):\n    terminated = False\n    steps = 0\n    episode_returns = 0 \n    state, info = env.reset(seed=42)\n    \n    while (not terminated and steps <= max_actions):\n        action = epsilon_greedy(state)\n        next_state, reward, terminated, truncated, info = env.step(action)\n        steps += 1\n        update_q_table(state, action, reward, next_state)\n        state = next_state\n        episode_returns += reward\n    \n    episode_total_reward[i] = episode_returns\n    epsilon= max(min_epsilon, epsilon*epsilon_decay)\n    \n    # Optionally print progress\n    if (i + 1) % 100 == 0:\n        print(f\"Episode {i+1}/{num_episodes}, Average reward: {np.mean(episode_total_reward[max(0, i-99):i+1]):.2f}\")\n\nprint(\"Average reward per random episode: \", np.mean(episode_total_reward))\nprint(\"\\nLearned Q table: \\n\", q_table)","metadata":{"executionCancelledAt":null,"executionTime":1029,"lastExecutedAt":1741641360443,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def update_q_table(state, action, reward, next_state):\n    q_table[state, action] = (1 - learning_rate) * q_table[state, action] + learning_rate * (reward + gamma * np.max(q_table[next_state]))\n\nepisode_total_reward = np.zeros(num_episodes)\nsave_frequency = 100 \n\nfor i in range(num_episodes):\n    terminated = False\n    steps = 0\n    episode_returns = 0 \n    state, info = env.reset(seed=42)\n    \n    while not terminated and steps <= max_actions:\n        action = env.action_space.sample()  \n        next_state, reward, terminated, truncated, info = env.step(action)\n        steps += 1\n        \n        if len(frames) < 1000 and steps % max_actions == 0: \n            frames.append(env.render())\n        \n        update_q_table(state, action, reward, next_state)\n        state = next_state\n        episode_returns += reward\n    \n    episode_total_reward[i] = episode_returns\n    \n    # Optionally print progress\n    if (i + 1) % 100 == 0:\n        print(f\"Episode {i+1}/{num_episodes}, Average reward: {np.mean(episode_total_reward[max(0, i-99):i+1]):.2f}\")\n\nprint(\"Average reward per random episode: \", np.mean(episode_total_reward))\nprint(\"\\nLearned Q table: \\n\", q_table)","lastExecutedByKernel":"03387f2a-fafb-4ab9-b67d-404d4d97d654","outputsMetadata":{"0":{"height":248,"type":"stream"}}},"id":"bbc86a7b-9bf9-4ec5-bc88-4098a70e392e","cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Episode 100/100, Average reward: -387.56\nAverage reward per random episode:  -387.56\n\nLearned Q table: \n [[  0.       0.       0.       0.       0.       0.    ]\n [  0.       0.       0.       0.       0.       0.    ]\n [ -0.99    -0.9     -0.9     -1.728   -0.99   -10.629 ]\n ...\n [  0.       0.       0.       0.       0.       0.    ]\n [  0.      -1.8648  -0.99     0.      -9.9      0.    ]\n [  0.       0.       0.       0.       0.       0.    ]]\n"}]},{"source":"for i in range(16): # Execute maximum 16 moves\n    action = policy[state] \n    state, reward, terminated, truncated, info = env.step(action)\n    episode_total_reward += reward\n    frames.append(env.render())\n    if terminated:\n      break    ","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1741641360491,"lastExecutedByKernel":"03387f2a-fafb-4ab9-b67d-404d4d97d654","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# policy = env.get_policy()"},"cell_type":"code","id":"94d1f28f-b146-4c88-8258-3964720d55c7","outputs":[],"execution_count":4},{"source":"# Once you are done, run this cell to visualize the agent's behavior through the episode\n# Save frames as a GIF\nimport imageio\nfrom IPython.display import Image, display\n\nimageio.mimsave('taxi_agent_behavior.gif', frames, fps=5)\n\n# Display GIF\ngif_path = \"taxi_agent_behavior.gif\" \ndisplay(Image(filename=gif_path))","metadata":{"executionCancelledAt":null,"executionTime":2877,"lastExecutedAt":1741641363368,"lastExecutedByKernel":"03387f2a-fafb-4ab9-b67d-404d4d97d654","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Once you are done, run this cell to visualize the agent's behavior through the episode\n# Save frames as a GIF\nimport imageio\nfrom IPython.display import Image, display\n\nimageio.mimsave('taxi_agent_behavior.gif', frames, fps=5)\n\n# Display GIF\ngif_path = \"taxi_agent_behavior.gif\" \ndisplay(Image(filename=gif_path))"},"id":"93ad05af-2e0f-4ada-b368-fabb0715bb30","cell_type":"code","execution_count":5,"outputs":[{"ename":"Error","evalue":"Output too large to be added to notebook","output_type":"error","traceback":[]}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}